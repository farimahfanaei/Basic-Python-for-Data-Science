{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest from Scratch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, x, y, n_trees, n_features, sample_sz, depth=10, min_leaf=5):\n",
    "        np.random.seed(12)\n",
    "        if n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(x.shape[1]))\n",
    "        elif n_features == 'log2':\n",
    "            self.n_features = int(np.log2(x.shape[1]))\n",
    "        else:\n",
    "            self.n_features = n_features\n",
    "        print(self.n_features, \"sha: \",x.shape[1])    \n",
    "        self.x, self.y, self.sample_sz, self.depth, self.min_leaf  = x, y, sample_sz, depth, min_leaf\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
    "\n",
    "    def create_tree(self):\n",
    "        idxs = np.random.permutation(len(self.y))[:self.sample_sz]\n",
    "        f_idxs = np.random.permutation(self.x.shape[1])[:self.n_features]\n",
    "        return DecisionTree(self.x.iloc[idxs], self.y[idxs], self.n_features, f_idxs,\n",
    "                    idxs=np.array(range(self.sample_sz)),depth = self.depth, min_leaf=self.min_leaf)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.mean([t.predict(x) for t in self.trees], axis=0)\n",
    "\n",
    "def std_agg(cnt, s1, s2): return math.sqrt((s2/cnt) - (s1/cnt)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n_trees :\n",
    "number of uncorrelated trees to create the random forest.\n",
    "### n_features:\n",
    "the number of features to sample and pass onto each tree, this is where feature bagging happens. It can either be sqrt, log2 or an integer. \n",
    "### sample_size: \n",
    "the number of rows randomly selected and passed onto each tree.\n",
    "### depth: \n",
    "depth of each decision tree. Higher depth means more number of splits which increases the over fitting tendency of each tree\n",
    "### min_leaf:\n",
    "minimum number of rows required in a node to cause further split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  __init__: \n",
    "constructor simply defining the random forest with help of our parameters and creating the required number of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, x, y, n_features, f_idxs,idxs,depth=10, min_leaf=5):\n",
    "        self.x, self.y, self.idxs, self.min_leaf, self.f_idxs = x, y, idxs, min_leaf, f_idxs\n",
    "        self.depth = depth\n",
    "        self.n_features = n_features\n",
    "        self.n, self.c = len(idxs), x.shape[1]\n",
    "        \n",
    "        # each decision tree predicts a value which is the average of all the rows it’s holding. \n",
    "        #The variable self.val holds the prediction for each node of the tree.\n",
    "        self.val = np.mean(y[idxs])\n",
    "        \n",
    "        ## the score is set to infinity for our node because we haven’t made any splits yet\n",
    "        #thus our in-existent split is infinitely bad, indicating that any split will be better than no split.\n",
    "        self.score = float('inf')\n",
    "        \n",
    "        #make the first split\n",
    "        self.find_varsplit()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        #Will make it recursive later\n",
    "        for i in self.f_idxs: self.find_better_split(i)\n",
    "       \n",
    "\n",
    "   \n",
    "    #calculate var_idx index and finds the best possible split in a certain column,\n",
    "    def find_better_split(self, var_idx):\n",
    "        #Lets write it later\n",
    "        pass\n",
    "    \n",
    "\n",
    "        for i in range(0,self.n-self.min_leaf-1):\n",
    "            xi,yi = sort_x[i],sort_y[i]\n",
    "            lhs_cnt += 1; rhs_cnt -= 1\n",
    "            lhs_sum += yi; rhs_sum -= yi\n",
    "            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n",
    "            if i<self.min_leaf or xi==sort_x[i+1]:\n",
    "                continue\n",
    "\n",
    "            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n",
    "            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n",
    "            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n",
    "            if curr_score<self.score: \n",
    "                self.var_idx,self.score,self.split = var_idx,curr_score,xi\n",
    "\n",
    "                \n",
    "    #A property decorator to return the name of the column we’re splitting over. var_idx is the index of this column\n",
    "    @property\n",
    "    def split_name(self): return self.x.columns[self.var_idx]\n",
    "    \n",
    "    \n",
    "    #A property decorator to return the column at index var_idx with elements at indices given by indxs variable.\n",
    "    @property\n",
    "    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n",
    "\n",
    "    \n",
    "    #A leaf node is the one that has never made a split thus it has a score of infinite\n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf') or self.depth <= 0 \n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "\n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf: return self.val\n",
    "        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n",
    "        return t.predict_row(xi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indxs: \n",
    "this parameter exists to keep track of which indices of the original set goes to the right and which goes to the left tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_leaf: \n",
    "minimum row samples required at a leaf node to be able to cause a split. \n",
    "### depth:\n",
    "Max depth or max number of splits possible within each tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
