{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "Hit Enter to continue: \n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> stopwords\n",
      "Command 'stopwords' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> stopwords\n",
      "    Downloading package stopwords to\n",
      "        C:\\Users\\farimah\\AppData\\Roaming\\nltk_data...\n",
      "      Unzipping corpora\\stopwords.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "#nltk.download_shell()\n",
    "#here to see a list of all the packages we press l  \n",
    "#then to download the stopwords we peress d and then as an identifier for our package we type stopwords\n",
    "# then once you are done, to quit the shell you press q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "from nltk.corpus import stopwords \n",
    "import re \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow a general method twhich can be used fpr text preprocessing in NLP : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_preprocessing(doc):\n",
    "    \"\"\"\n",
    "    1. convert text to lowercase and remove numbers (if neccassery)\n",
    "    2. remove the punctuations( [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]: )\n",
    "    3. remove the stopwords \n",
    "    4. return list of cean text words \n",
    "    \n",
    "    \"\"\"\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'\\d+', '', doc)\n",
    "    no_punc = [char for char in doc if char not in string.punctuation]\n",
    "    no_punc = ''.join(no_punc)\n",
    "    ## or we can use the following mehthod instead :\n",
    "    # no_punc = doc.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in no_punc.split() if word.lower not in stop_words]\n",
    "    #instead of usinf split we could also use word_tokenize method as the following : \n",
    "    #  tokens = word_tokenize(no_punc) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "def stem_func(doc):\n",
    "    stemmer= PorterStemmer()\n",
    "    doc=word_tokenize(doc)\n",
    "    return [word for word in stemmer.stem(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "steps for using the bag-of-words model:\n",
    "\n",
    "       1.Count how many times does a word occur in each message (Known as term frequency)\n",
    "\n",
    "       2.Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "\n",
    "       3.Normalize the vectors to unit length, to abstract from the original text length (L2 norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection', sep='\\t',\n",
    "                           names=[\"label\", \"message\"])\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['length'] = messages['message'].apply(len)\n",
    "messages.length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000001FF06AA2630>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000001FF06D46978>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages.hist(column='length', by='label', bins=50,figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8707\n"
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=txt_preprocessing).fit(messages['message'])\n",
    "\n",
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "message4 = messages['message'][3]\n",
    "print(message4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 222)\t1\n",
      "  (0, 1005)\t1\n",
      "  (0, 2104)\t1\n",
      "  (0, 2126)\t1\n",
      "  (0, 3337)\t1\n",
      "  (0, 6361)\t2\n",
      "  (0, 6782)\t1\n",
      "  (0, 7452)\t1\n",
      "  (0, 7809)\t2\n",
      "(1, 8707)\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[7452])\n",
    "print(bow_transformer.get_feature_names()[7809])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (5572, 8707)\n",
      "Amount of Non-Zero occurences:  76084\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', messages_bow.shape)\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To grab the sparcity of the data as the following, which is the number of the non zeros messages versus the total number of messges , to check homany zeros are in our matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0.15682441807554565\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF:\n",
    "Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:\n",
    "\n",
    "TF(t) =(Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF: \n",
    "Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:\n",
    "\n",
    "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tfidf = TfidfTransformer().fit(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf4 = tfidf.transform(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x8707 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7809)\t0.3049003006354209\n",
      "  (0, 7452)\t0.22000702594510108\n",
      "  (0, 6782)\t0.18806605002983492\n",
      "  (0, 6361)\t0.5363911547427017\n",
      "  (0, 3337)\t0.4451821820956418\n",
      "  (0, 2126)\t0.3215992335944762\n",
      "  (0, 2104)\t0.29625180896634806\n",
      "  (0, 1005)\t0.2724723119985057\n",
      "  (0, 222)\t0.26819557737135086\n"
     ]
    }
   ],
   "source": [
    "# the following is another term (tf-idf) for message4\n",
    "\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check the document frequency for a word in the document, you need to paas your vocabulary to the obj of Tfidftransformer() : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.864488671875974"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idf_[bow_transformer.vocabulary_['hello']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert the whole BOW corpus into the TF-IDF corpus at once : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 8707)\n"
     ]
    }
   ],
   "source": [
    "messages_tfidf = tfidf.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Machine learning algorithems and start training the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ham\n",
      "expected: ham\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detect_model.predict(tfidf4)[0])\n",
    "print('expected:', messages.label[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_detect_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'spam', ..., 'ham', 'ham', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction = spam_detect_model.predict(messages_tfidf)\n",
    "final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.98      4825\n",
      "        spam       1.00      0.79      0.88       747\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      5572\n",
      "   macro avg       0.98      0.89      0.93      5572\n",
      "weighted avg       0.97      0.97      0.97      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(messages['label'], final_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we did not split the dataset which is not a correct way of evaluation in machine learning , but if we had already done that we should repeat all the steps from countvectorization to transformation , tf-idf, fiting it etc... then we could apply the machine leaning algorithm on training data, the folloaing is the way for train test spliting in text data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457 1115 5572\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "msg_train, msg_test, label_train, label_test = \\\n",
    "train_test_split(messages['message'], messages['label'], test_size=0.2)\n",
    "\n",
    "print(len(msg_train), len(msg_test), len(msg_train) + len(msg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But Scikit learn has a data pipeline feature which save our time not to repeat all those steps for each set, so we can summarise all the steps we ust did into a pipeline(it takes the steps arguments so you pass a list of the things you want to do) to prevent repeating everything for each set :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=txt_preprocessing)),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('bow', CountVectorizer(analyzer=<function txt_preprocessing at 0x000001FF07436378>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), prepro...f=False, use_idf=True)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#So here we only paa the training data to the pipe line and it will do all the steps of preprocessing and classification :\n",
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to here it will do the prediction part on the testing set :\n",
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.95      0.97       996\n",
      "        spam       0.69      1.00      0.82       119\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1115\n",
      "   macro avg       0.85      0.97      0.90      1115\n",
      "weighted avg       0.97      0.95      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
